from fastapi import FastAPI, Request, HTTPException, status
from fastapi.responses import JSONResponse
import json
import os
from github import Github
import requests
import datetime

GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
REPO_NAME = "generated-apps"

app = FastAPI(title="LLM Task Processor API")



def sanitize_branch_name(name: str) -> str:
    # Replace invalid characters with underscores
    name = re.sub(r'[^a-zA-Z0-9_\-\/]', '_', name)
    # Remove leading/trailing slashes or dots
    name = name.strip('/.')
    return name



def push_files_to_github(files: dict, task_name: str, commit_message="Generated by AI"):
    if not GITHUB_TOKEN:
        raise ValueError("GITHUB_TOKEN not set in environment")
    
    g = Github(GITHUB_TOKEN)
    repo = g.get_user().get_repo("Project1")

    # Generate unique branch name
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    branch_name = f"{sanitize_branch_name(task_name)}_{timestamp}"

    # Get main branch commit SHA
    main_branch = repo.get_branch("main")
    repo.create_git_ref(ref=f"refs/heads/{branch_name}", sha=main_branch.commit.sha)

    # Push all files to new branch
    for filename, content in files.items():
        path = f"ai_generated_files/{filename}"  # optional folder
        try:
            existing_file = repo.get_contents(path, ref=branch_name)
            repo.update_file(existing_file.path, commit_message, content, existing_file.sha, branch=branch_name)
        except:
            repo.create_file(path, commit_message, content, branch=branch_name)

    return f"https://github.com/{repo.full_name}/tree/{branch_name}"

def parse_gpt_response(response_text: str):
    files = {}
    current_file = None
    content = []
    
    for line in response_text.splitlines():
        if line.startswith("# File:"):
            if current_file:
                files[current_file] = "\n".join(content)
            current_file = line.replace("# File:", "").strip()
            content = []
        else:
            content.append(line)
    
    if current_file:
        files[current_file] = "\n".join(content)
    
    return files




@app.post("/receive")
async def receive_task_endpoint(request: Request):


    aipipe_token = os.environ.get('AIPIPE_TOKEN')
    if not aipipe_token:
        # FastAPI way to handle server error
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail='Server configuration error: AIPIPE_TOKEN environment variable is not set.'
        )

    try:
        # 2. Parse the incoming JSON body asynchronously
        request_body = await request.json()
    except json.JSONDecodeError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail='Invalid JSON format in request body.'
        )

    task_description = request_body.get('brief','')
    task = request_body.get('brief')
    task_description += "\n\nPlease follow these rules strictly:\n" \
                    "1. Only output code, no explanations.\n" \
                    "2. Each new file must start with '# File: filename.ext'.\n" \
                    "3. Do not include any markdown or instructions outside the code.\n" \
                    "4. Do not wrap code in triple backticks.\n" \
                    "5. Name all files with appropriate extensions.\n" \
                    "6. also make a readme.md file\n"
    specific_model = request_body.get('specificModel', 'gpt-4o')

    if not task_description:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail='Missing required field: "taskDescription" in the request body.'
        )

    try:
        AI_SYSTEM_PROMPT = "You are an expert AI task execution agent. Your role is to carefully analyze the user's request and provide a clear, detailed, and actionable response that fully completes the task."

        payload = {
            'model': specific_model,
            'messages': [
                {'role': 'system', 'content': AI_SYSTEM_PROMPT},
                {'role': 'user', 'content': task_description}
            ],
            'temperature': 0.7,
            'max_tokens': 2048
        }
        AIPIPE_ENDPOINT = 'https://aipipe.org/openai/v1/chat/completions'

        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {aipipe_token}'
        }

        api_response = requests.post(AIPIPE_ENDPOINT, headers=headers, json=payload, timeout=60)
        api_data = api_response.json()
        gpt_text = api_data['choices'][0]['message']['content']
        files = parse_gpt_response(gpt_text)
        repo_url = push_files_to_github(files,task , commit_message=f"Task: {task_description[:30]}")

        # 5. Handle API response
        if not api_response.ok:
            print(f'AI Pipe API Error: {api_data}')
            error_details = api_data.get('error', {}).get('message', 'Unknown AI Pipe error')
            
            # Use upstream status code if available, otherwise default to 500
            error_status_code = api_response.status_code if api_response.status_code >= 400 else status.HTTP_500_INTERNAL_SERVER_ERROR

            raise HTTPException(
                status_code=error_status_code,
                detail={
                    'error': 'Failed to complete AI task.',
                    'details': error_details
                }
            )

    
        return JSONResponse(
            content={
                'taskExecuted': True,
                'model': api_data.get('model'),
                'file': files
            },
            status_code=status.HTTP_200_OK
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f'Server Function Execution Error: {e}')
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f'Internal Server Error during task processing: {str(e)}'
        )

@app.get("/")
async def root():
    return {
        "service": "LLM Task Processor API",
        "status": "OK",
        "endpoint": "/receive (POST)"
    }